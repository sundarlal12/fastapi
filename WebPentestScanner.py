# #!/usr/bin/env python3
# # WebPentestScanner.py
# # Author: Sundar Lal Baror (single-file local-folder scanner)

# import tiktoken
# import string
# import secrets
# import json
# import psycopg2
# from psycopg2.extras import RealDictCursor
# from risk_analyzer import RiskAnalyzer
# import sys
# import os
# import urllib.parse
# import base64
# from json.decoder import JSONDecodeError
# import fnmatch
# import requests
# from dotenv import load_dotenv
# from concurrent.futures import ThreadPoolExecutor, as_completed
# from datetime import datetime
# import yaml
# import regex as re
# from pathlib import Path
# import openai
# import time
# import random

# # ---------------- Env / Globals ----------------
# MAX_RETRIES = 4
# RETRY_DELAY = 3

# load_dotenv()
# SITE_URL = os.getenv("SITE_URL")
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# OSV_API_URL = "https://api.osv.dev/v1/query"
# MODEL = "gpt-4o"
# client = openai.OpenAI(api_key=OPENAI_API_KEY)
# EMAIL = ""
# TOKEN = ""
# DB_CONFIG = {
#     "user": os.getenv("DB_USERNAME"),
#     "password": os.getenv("DB_PASSWORD"),
#     "host": os.getenv("DB_HOST"),
#     "dbname": os.getenv("DB_NAME"),
#     "port": int(os.getenv("DB_PORT", "5432"))
# }
# DB_PORT = int(os.getenv("DB_PORT", "5432"))
# DATABASE_URL = os.getenv("DATABASE_URL")
# DB_SCHEMA = os.getenv("DB_SCHEMA", "sastcode_schema")

# CATEGORIES = [
#     "dead_code", "docstring", "malicious_code",
#     "owasp_security", "secrets", "smelly_code"
# ]

# CATEGORY_TABLE_MAP = {
#     "dead_code": "dead_code_info",
#     "docstring": "docstring_info",
#     "malicious_code": "malicious_code_info",
#     "owasp_security": "owasp_security_info",
#     "secrets": "secrets_info",
#     "smelly_code": "smelly_code_info"
# }

# TARGET_EXTS = (".js", ".mjs", ".cjs")
# PREFER_SUFFIX = "_beautified.js"
# MAX_THREADS = 4  # throttle LLM calls

# # ---------------- DB / Prompt ----------------
# def get_db_connection():
#     try:
#         conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor, connect_timeout=5, sslmode="require")
#         with conn.cursor() as cur:
#             cur.execute(f"SET search_path TO {DB_SCHEMA};")
#         return conn
#     except Exception as e:
#         raise RuntimeError(f"DB unreachable: {e}")

# def load_prompt_template(path="review_prompt.yml"):
#     with open(path, "r") as file:
#         yml = yaml.safe_load(file)
#         return yml["review_template"]

# # ---------------- JSON Extraction ----------------
# def extract_json_from_response(text):
#     def extract_json_blocks(text_):
#         blocks, stack = [], []
#         start = -1
#         in_string = False
#         escape = False
#         in_bad_practice = False

#         for i, ch in enumerate(text_):
#             if not in_string and text_[i:i+13] == '"bad_practice"':
#                 in_bad_practice = True

#             if not in_string:
#                 if ch == '{':
#                     if not stack:
#                         start = i
#                     stack.append(ch)
#                 elif ch == '}':
#                     if stack:
#                         stack.pop()
#                         if not stack and start != -1:
#                             blocks.append(text_[start:i+1])
#                             start = -1
#                 elif ch == '"':
#                     in_string = True
#             else:
#                 if escape:
#                     escape = False
#                 elif ch == '\\':
#                     escape = True
#                 elif ch == '"':
#                     in_string = False
#                     in_bad_practice = False

#                 if in_bad_practice and ch == '"' and not escape:
#                     end_quote = text_.find('"', i+1)
#                     if end_quote == -1:
#                         end_quote = text_.find('}', i+1)
#                         if end_quote == -1:
#                             end_quote = len(text_)-1
#                     text_ = text_[:end_quote] + '"' + text_[end_quote:]
#                     in_string = False
#                     in_bad_practice = False
#         return blocks

#     json_blocks = extract_json_blocks(text)
#     if not json_blocks:
#         print("‚ùå No valid JSON blocks found.")
#         return None

#     def sanitize_and_parse(block):
#         try:
#             parsed = json.loads(block)
#             if isinstance(parsed, dict):
#                 for key, value in parsed.items():
#                     if key == 'malicious_code':
#                         for path, findings in value.items():
#                             for finding in findings:
#                                 if 'bad_practice' in finding:
#                                     finding['bad_practice'] = urllib.parse.quote(finding['bad_practice'])
#             return parsed
#         except JSONDecodeError:
#             pass

#         try:
#             def encode_bad_practice(m):
#                 content = m.group(1)
#                 try:
#                     unescaped = bytes(content, 'utf-8').decode('unicode-escape')
#                     return f'"bad_practice": "{urllib.parse.quote(unescaped)}"'
#                 except:
#                     return f'"bad_practice": "{urllib.parse.quote(content)}"'

#             sanitized = re.sub(
#                 r'"bad_practice"\s*:\s*"((?:\\"|[^"])*?)"(?=\s*[,}])',
#                 encode_bad_practice,
#                 block,
#                 flags=re.DOTALL
#             )
#             sanitized = re.sub(r',\s*([}\]])', r'\1', sanitized)
#             sanitized = re.sub(r'([{,])\s*([^"\s]\S*?)\s*:', r'\1"\2":', sanitized)
#             return json.loads(sanitized)
#         except JSONDecodeError as e:
#             print(f"‚ö†Ô∏è JSON decode error after sanitization: {e}")
#             return None

#     for b in json_blocks:
#         parsed = sanitize_and_parse(b)
#         if parsed:
#             return parsed
#     print("‚ùå Failed to parse any JSON blocks")
#     return None

# # ---------------- Chunking / Analysis ----------------
# def split_code_to_chunks_with_line_numbers(code_text, max_chunk_tokens=3000, model="gpt-4o"):
#     encoder = tiktoken.encoding_for_model(model)
#     lines = code_text.splitlines()
#     chunks, current_chunk_lines = [], []
#     current_token_count = 0
#     line_offset = 1

#     for line in lines:
#         encoded_line = encoder.encode(line + "\n")
#         line_token_len = len(encoded_line)
#         if current_token_count + line_token_len > max_chunk_tokens and current_chunk_lines:
#             chunk_text = "\n".join(current_chunk_lines)
#             chunks.append((line_offset - len(current_chunk_lines), chunk_text))
#             current_chunk_lines = []
#             current_token_count = 0
#         current_chunk_lines.append(f"{line_offset}: {line}")
#         current_token_count += line_token_len
#         line_offset += 1

#     if current_chunk_lines:
#         chunk_text = "\n".join(current_chunk_lines)
#         chunks.append((line_offset - len(current_chunk_lines), chunk_text))
#     return chunks

# def analyze_code(code_content, prompt_template, github_username="", repo_name="", branch_name="main", repo_file_path=""):
#     chunks = split_code_to_chunks_with_line_numbers(code_content)
#     encoder = tiktoken.encoding_for_model("gpt-4o")

#     for i, (start_line, chunk_text) in enumerate(chunks):
#         token_count = len(encoder.encode(chunk_text))
#         print(f"Chunk {i+1} tokens: {token_count}")

#         prompt = f"{prompt_template}\nAnalyze the following code. Each line is prefixed with a line number for reference:\n\n{chunk_text}"

#         for retry_count in range(4):
#             try:
#                 print(f"üì§ Sending chunk {i+1}/{len(chunks)}")
#                 response = client.chat.completions.create(
#                     model=MODEL,
#                     messages=[{"role": "user", "content": prompt}],
#                     temperature=0.2,
#                     top_p=0.1,
#                     max_tokens=8192
#                 )
#                 content = response.choices[0].message.content
#                 chunk_result = extract_json_from_response(content)
#                 if not isinstance(chunk_result, dict):
#                     print(f"‚ö†Ô∏è Skipping chunk {i+1}: Invalid JSON")
#                     continue
#                 chunk_result["file_path"] = repo_file_path

#                 categorize_and_save(
#                     chunk_result,
#                     github_username=github_username,
#                     repo_name=repo_name,
#                     branch_name=branch_name
#                 )
#                 time.sleep(3)
#                 break
#             except Exception as e:
#                 if "429" in str(e) or "rate limit" in str(e).lower():
#                     wait = 3 * (retry_count + 1)
#                     print(f"‚ö†Ô∏è Rate limit. Retrying after {wait}s")
#                     time.sleep(wait)
#                 else:
#                     print(f"‚ùå Error in chunk {i+1}: {e}")
#                     break
#     return {"status": "chunks_processed"}

# # ---------------- Save to DB ----------------
# def categorize_and_save(data, github_username, repo_name, branch_name="main", email=EMAIL, platform="WebApp"):
#     result = data.get("result", data)
#     print(result)
#     repo_file_path = data.get("file_path", "")
#     created_at = datetime.now()
#     risk_analyzer = RiskAnalyzer()

#     def generate_mongodb_id(repo_name_, line_number_, created_at_):
#         alphabet = string.ascii_lowercase + string.digits
#         random_part = ''.join(secrets.choice(alphabet) for _ in range(16))
#         timestamp_hex = format(int(created_at_.timestamp()), '08x')
#         repo_hash = format(abs(hash(repo_name_)) % (10**8), '06x')
#         return f"{timestamp_hex}{repo_hash}{random_part}"[:24]

#     def handle_array_field(field_data):
#         if isinstance(field_data, list):
#             return json.dumps(field_data)
#         return field_data

#     def normalize_issue(issue, category):
#         if not isinstance(issue, dict):
#             return None

#         line_num = issue.get("line_number", 0)
#         mongodb_beacon_id = generate_mongodb_id(repo_name, line_num, created_at)
#         risk_analysis = risk_analyzer.analyze_issue_risk(issue)

#         ai_severity = issue.get("severity", "Medium")
#         risk_level = risk_analysis["risk_level"]
#         severity_order = {"Info": 0, "Low": 1, "Medium": 2, "High": 3, "Critical": 4}
#         ai_level = severity_order.get(ai_severity, 1)
#         risk_level_num = severity_order.get(risk_level, 1)
#         final_severity = ai_severity if ai_level <= risk_level_num else risk_level

#         base_issue = {
#             "username": github_username,
#             "email": email,
#             "platform": platform,
#             "repo_name": repo_name,
#             "file_path": repo_file_path,
#             "line_number": issue.get("line_number"),
#             "vulnerability_type": issue.get("vulnerability_type") or issue.get("issue") or issue.get("issue_type"),
#             "cwe": issue.get("cwe", "N/A"),
#             "cve": issue.get("cve", ""),
#             "severity": final_severity,
#             "risk_score": risk_analysis["risk_score"],
#             "risk_level": risk_analysis["risk_level"],
#             "short_description": issue.get("description") or issue.get("short_description", ""),
#             "suggested_fix": issue.get("suggested_fix", "Review the code and apply necessary validation/sanitization."),
#             "created_at": created_at,
#             "bad_practice": issue.get("bad_practice") if category in ["smelly_code", "malicious_code"] else None,
#             "good_practice": issue.get("good_practice") if category in ["smelly_code", "malicious_code"] else None,
#             "issueId": issue.get("issue_id") or issue.get("id") or mongodb_beacon_id,
#             "branch": branch_name,
#             "owasp_2017": issue.get("owasp_2017"),
#             "owasp_2021": issue.get("owasp_2021"),
#             "reproduction_steps": handle_array_field(issue.get("reproduction_steps")),
#             "medium_vapt_summary": issue.get("medium_vapt_summary"),
#             "impact": handle_array_field(issue.get("impact")),
#             "remediation": handle_array_field(issue.get("remediation")),
#             "reference": handle_array_field(issue.get("references"))
#         }

#         if category == "owasp_security":
#             base_issue["bad_practice"] = issue.get("vulnerable_code", "")
#             base_issue["good_practice"] = issue.get("patched_code", "")
#             base_issue["reproduction_steps"] = base_issue["reproduction_steps"] or json.dumps([])
#             base_issue["impact"] = base_issue["impact"] or json.dumps([])
#             base_issue["remediation"] = base_issue["remediation"] or json.dumps([])
#             base_issue["reference"] = base_issue["reference"] or json.dumps([])

#         return base_issue

#     conn = get_db_connection()
#     cursor = conn.cursor()
#     try:
#         for category in CATEGORIES:
#             section = result.get(category)
#             if not section:
#                 continue

#             table_name = CATEGORY_TABLE_MAP[category]
#             issues = []

#             if isinstance(section, dict):
#                 for _, issue_list in section.items():
#                     for i in issue_list:
#                         normalized = normalize_issue(i, category)
#                         if normalized:
#                             issues.append(normalized)
#             elif isinstance(section, list):
#                 for i in section:
#                     normalized = normalize_issue(i, category)
#                     if normalized:
#                         issues.append(normalized)

#             if not issues:
#                 continue

#             keys = issues[0].keys()
#             fields = ", ".join(keys)
#             placeholders = ", ".join(["%s"] * len(keys))
#             insert_query = f"INSERT INTO {table_name} ({fields}) VALUES ({placeholders})"

#             for issue in issues:
#                 cursor.execute(insert_query, list(issue.values()))

#             print(f"‚úÖ Inserted {len(issues)} issues into {table_name}")

#         conn.commit()
#     except Exception as e:
#         conn.rollback()
#         print(f"‚ùå Error inserting issues: {e}")
#     finally:
#         cursor.close()
#         conn.close()

# # ---------------- Local Folder Helpers ----------------
# def list_js_files(scan_folder: str):
#     """Return list of JS-like files; prefer *_beautified.js if any exist."""
#     scan_folder = os.path.abspath(scan_folder)
#     beautified, plain = [], []
#     for root, _, files in os.walk(scan_folder):
#         for fn in files:
#             low = fn.lower()
#             if not low.endswith(TARGET_EXTS):
#                 continue
#             full = os.path.join(root, fn)
#             if low.endswith(PREFER_SUFFIX):
#                 beautified.append(full)
#             else:
#                 plain.append(full)
#     return beautified if beautified else plain

# def read_text(path: str) -> str:
#     with open(path, "r", encoding="utf-8", errors="ignore") as f:
#         return f.read()

# def process_local_file(filepath: str, prompt_template: str, username: str, repo_name: str, branch_name: str):
#     try:
#         code = read_text(filepath)
#         rel_path = Path(filepath).as_posix()
#         print(f"ü§ñ Analyzing: {rel_path}")

#         analysis = analyze_code(
#             code_content=code,
#             prompt_template=prompt_template,
#             github_username=username,
#             repo_name=repo_name,
#             branch_name=branch_name,
#             repo_file_path=rel_path,
#         )

#         if not isinstance(analysis, dict):
#             print(f"‚ùå Skipping {rel_path}: Invalid result")
#             return f"‚ùå Invalid: {rel_path}"

#         analysis["file_path"] = rel_path
#         categorize_and_save(
#             analysis,
#             github_username=username,
#             repo_name=repo_name,
#             branch_name=branch_name,
#         )

#         time.sleep(random.uniform(2.0, 5.0))
#         return f"‚úÖ Processed: {rel_path}"
#     except Exception as e:
#         return f"‚ùå Error: {rel_path} ({e})"

# # ---------------- Scan Status ----------------
# def update_scan_status(scan_id, status="completed"):
#     conn = get_db_connection()
#     cursor = conn.cursor()
#     try:
#         try:
#             scan_id_int = int(str(scan_id))
#         except ValueError:
#             scan_id_int = None
#         if scan_id_int is None:
#             # fallback if scan_id is not numeric; store in a text column if you have one
#             cursor.execute("""
#                 UPDATE sastcode_schema.scan_status
#                 SET status = %s, finished_at = NOW()
#                 WHERE scan_id::text = %s
#             """, (status, str(scan_id)))
#         else:
#             cursor.execute("""
#                 UPDATE sastcode_schema.scan_status
#                 SET status = %s, finished_at = NOW()
#                 WHERE scan_id = %s
#             """, (status, scan_id_int))
#         conn.commit()
#         print(f"‚úÖ scan_status ‚Üí '{status}' for scan_id {scan_id}")
#     except Exception as e:
#         conn.rollback()
#         print(f"‚ùå Error updating scan_status: {e}")
#     finally:
#         cursor.close()
#         conn.close()

# # ---------------- Main ----------------
# def main():
#     if len(sys.argv) < 3:
#         print("Usage: python WebPentestScanner.py <username> <scan_id> [domain_name]")
#         sys.exit(1)

#     username = sys.argv[1]
#     scan_id = sys.argv[2]
#     domain_name = sys.argv[3] if len(sys.argv) > 3 else None

#     scan_folder = os.path.abspath(scan_id)
#     repo_name = domain_name if domain_name else scan_id
#     branch = "main"

#     print("üîé Local JS analysis")
#     print(f"  user      : {username}")
#     print(f"  scan_id   : {scan_id}")
#     print(f"  repo_name : {repo_name}")
#     print(f"  folder    : {scan_folder}")
#     print(f"  branch    : {branch}")

#     try:
#         files = list_js_files(scan_folder)
#         if not files:
#             print("‚ö†Ô∏è No JS files found to analyze in this scan folder.")
#             update_scan_status(scan_id, status="completed")
#             return

#         prompt_template = load_prompt_template()
#         threads = min(MAX_THREADS, max(1, len(files)))
#         print(f"üöÄ Processing {len(files)} file(s) with {threads} thread(s)...")

#         with ThreadPoolExecutor(max_workers=threads) as ex:
#             futs = [ex.submit(process_local_file, f, prompt_template, username, repo_name, branch) for f in files]
#             for fut in as_completed(futs):
#                 print(fut.result())

#         print("‚úÖ All files processed.")
#         update_scan_status(scan_id, status="completed")

#     except Exception as e:
#         print(f"‚ùå Fatal error: {e}")
#         update_scan_status(scan_id, status="failed")
#         sys.exit(1)

# if __name__ == "__main__":
#     main()



#!/usr/bin/env python3
# WebPentestScanner.py ‚Äî Local folder analyzer with SCA + JS LLM analysis

import tiktoken
import string

import re as _re
import secrets
import json
import psycopg2
from psycopg2.extras import RealDictCursor
from risk_analyzer import RiskAnalyzer
import sys
import os
import urllib.parse
import base64
from json.decoder import JSONDecodeError
import fnmatch
import requests
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import yaml
import regex as re
from pathlib import Path
import xml.etree.ElementTree as ET
import openai
import time
import random


# ---------------- Env / Globals ----------------
MAX_RETRIES = 4
RETRY_DELAY = 3
MAX_THREADS = 4  # throttle LLM calls
TARGET_JS_EXTS = (".js", ".mjs", ".cjs")


# --- Allowlist / Exclusions (mirror of scanner.py) ---
ALLOWED_EXTENSIONS = {
    # JS/TS (we still only *analyze* JS via LLM, but this lets us filter cleanly)
    ".js", ".jsx", ".mjs", ".cjs", ".ts", ".tsx", ".mts", ".cts",
    # For completeness if you later widen local analysis
    ".json", ".txt", ".xml"
}

SPECIAL_SCA_FILES = {"package.json", "requirements.txt", "pom.xml"}

EXCLUDED_DIRS = {
    "node_modules", "vendor", "dist", "build", "out", "__MACOSX", ".next", ".nuxt"
}

EXCLUDED_PATTERNS = {
    # libs/minified/bundles
    "jquery*.js", "*.min.js", "*.bundle.js",
    "react*.js", "angular*.js", "vue*.js",
    "bootstrap*.js", "*bootstrap*.js",
    "popper*.js", "*popper*.js",
    "*.map", "*.d.ts",

    # path-aware (directory) patterns
    "node_modules/*", "vendor/*", "dist/*", "build/*", "out/*", "__MACOSX/*",
}


PREFER_SUFFIX = "_beautified.js"

load_dotenv()
SITE_URL = os.getenv("SITE_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OSV_API_URL = "https://api.osv.dev/v1/query"
MODEL = "gpt-4o"
client = openai.OpenAI(api_key=OPENAI_API_KEY)
EMAIL = ""
TOKEN = ""
DB_CONFIG = {
    "user": os.getenv("DB_USERNAME"),
    "password": os.getenv("DB_PASSWORD"),
    "host": os.getenv("DB_HOST"),
    "dbname": os.getenv("DB_NAME"),
    "port": int(os.getenv("DB_PORT", "5432"))
}
DATABASE_URL = os.getenv("DATABASE_URL")
DB_SCHEMA = os.getenv("DB_SCHEMA", "sastcode_schema")

CATEGORIES = [
    "dead_code", "docstring", "malicious_code",
    "owasp_security", "secrets", "smelly_code"
]
CATEGORY_TABLE_MAP = {
    "dead_code": "dead_code_info",
    "docstring": "docstring_info",
    "malicious_code": "malicious_code_info",
    "owasp_security": "owasp_security_info",
    "secrets": "secrets_info",
    "smelly_code": "smelly_code_info"
}

SCA_FILENAMES = {"package.json", "requirements.txt", "pom.xml"}

# ---------------- DB / Prompt ----------------
def get_db_connection():
    try:
        conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor, connect_timeout=5, sslmode="require")
        with conn.cursor() as cur:
            cur.execute(f"SET search_path TO {DB_SCHEMA};")
        return conn
    except Exception as e:
        raise RuntimeError(f"DB unreachable: {e}")

def load_prompt_template(path="review_prompt.yml"):
    with open(path, "r") as file:
        yml = yaml.safe_load(file)
        return yml["review_template"]

# ---------------- JSON Extraction ----------------
def extract_json_from_response(text):
    def extract_json_blocks(text_):
        blocks, stack = [], []
        start = -1
        in_string = False
        escape = False
        in_bad_practice = False

        for i, ch in enumerate(text_):
            if not in_string and text_[i:i+13] == '"bad_practice"':
                in_bad_practice = True

            if not in_string:
                if ch == '{':
                    if not stack:
                        start = i
                    stack.append(ch)
                elif ch == '}':
                    if stack:
                        stack.pop()
                        if not stack and start != -1:
                            blocks.append(text_[start:i+1])
                            start = -1
                elif ch == '"':
                    in_string = True
            else:
                if escape:
                    escape = False
                elif ch == '\\':
                    escape = True
                elif ch == '"':
                    in_string = False
                    in_bad_practice = False

                if in_bad_practice and ch == '"' and not escape:
                    end_quote = text_.find('"', i+1)
                    if end_quote == -1:
                        end_quote = text_.find('}', i+1)
                        if end_quote == -1:
                            end_quote = len(text_)-1
                    text_ = text_[:end_quote] + '"' + text_[end_quote:]
                    in_string = False
                    in_bad_practice = False
        return blocks

    json_blocks = extract_json_blocks(text)
    if not json_blocks:
        print("‚ùå No valid JSON blocks found.")
        return None

    def sanitize_and_parse(block):
        try:
            parsed = json.loads(block)
            if isinstance(parsed, dict):
                for key, value in parsed.items():
                    if key == 'malicious_code':
                        for path, findings in value.items():
                            for finding in findings:
                                if 'bad_practice' in finding:
                                    finding['bad_practice'] = urllib.parse.quote(finding['bad_practice'])
            return parsed
        except JSONDecodeError:
            pass

        try:
            def encode_bad_practice(m):
                content = m.group(1)
                try:
                    unescaped = bytes(content, 'utf-8').decode('unicode-escape')
                    return f'"bad_practice": "{urllib.parse.quote(unescaped)}"'
                except:
                    return f'"bad_practice": "{urllib.parse.quote(content)}"'

            sanitized = re.sub(
                r'"bad_practice"\s*:\s*"((?:\\"|[^"])*?)"(?=\s*[,}])',
                encode_bad_practice,
                block,
                flags=re.DOTALL
            )
            sanitized = re.sub(r',\s*([}\]])', r'\1', sanitized)
            sanitized = re.sub(r'([{,])\s*([^"\s]\S*?)\s*:', r'\1"\2":', sanitized)
            return json.loads(sanitized)
        except JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON decode error after sanitization: {e}")
            return None

    for b in json_blocks:
        parsed = sanitize_and_parse(b)
        if parsed:
            return parsed
    print("‚ùå Failed to parse any JSON blocks")
    return None



import fnmatch

def _is_excluded_path(full_path: str, base_name: str) -> bool:
    """
    Return True if the file should be skipped based on EXCLUDED_DIRS / EXCLUDED_PATTERNS.
    Checks both normalized full path and base name.
    """
    p = full_path.replace("\\", "/").lower()
    name = base_name.lower()

    # quick directory veto
    parts = [seg for seg in p.split("/") if seg]
    if any(d.lower() in parts for d in EXCLUDED_DIRS):
        return True

    # pattern veto (match both path and name)
    for pat in EXCLUDED_PATTERNS:
        lp = pat.lower()
        if "/" in lp:
            if fnmatch.fnmatch(p, lp):
                return True
        else:
            if fnmatch.fnmatch(name, lp) or fnmatch.fnmatch(p, lp):
                return True
    return False



# def normalize_repo_file_path(p: str) -> str:
#     """
#     Return only the filename (no directories).
#     If it's a JS file ending with _beautified.js, strip the _beautified.
#     """
#     name = Path(p).name
#     # strip only when it‚Äôs exactly before .js at the end
#     name = _re.sub(r"_beautified(?=\.js$)", "", name, flags=_re.IGNORECASE)
#     #print("filemname__-+",name)
#     return name

# def normalize_repo_file_path(p: str) -> str:
#     """
#     Given a path like: /full/path/scan_id_123/assets/js/file_beautified.js
#     ‚Üí return: assets/js/file.js
#     """
#     p = Path(p)

#     # Extract everything *after* the scan folder
#     parts = p.parts

#     # Find the first part that starts with scan_id_*
#     idx = next((i for i, x in enumerate(parts) if x.startswith("scan_id_")), None)

#     if idx is not None and idx + 1 < len(parts):
#         # keep everything after the scan_id_* folder
#         rel_parts = parts[idx + 1:]
#     else:
#         # fallback: take full filename only
#         rel_parts = parts[-1:]

#     rel_path = "/".join(rel_parts)

#     # remove `_beautified` if present
#     rel_path = _re.sub(r"_beautified(?=\.js$)", "", rel_path, flags=_re.IGNORECASE)
#     print("rel_path--->---->",rel_path)

#     return rel_path


# def normalize_repo_file_path(p: str) -> str:
#     """
#     Examples
#     - /.../scan_id_345678/assest:dfg:svcg:lazysizes.min.js
#         -> assest/dfg/svcg/lazysizes.min.js
#     - /.../scan_id_123/assets/js/file_beautified.js
#         -> assets/js/file.js
#     - /.../scan_id_matter/lazysizes.min.js
#         -> lazysizes.min.js
#     - /Users/sundarlalbaror/Downloads/hack/testapi/github/scan_id_kudowswell/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min_beautified.js
#         -> cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js
#     """
#     # Work with the raw string for reliable regex; normalize slashes
#     s = str(p).replace("\\", "/")
#     #print('lokja-oath---->',p)

#     # Grab everything AFTER the scan_id_* directory (case-insensitive)
#     m = _re.search(r"(?i)(?:^|/)scan_id_[^/]+/(.*)$", s)
#     if m:
#         rel = m.group(1)  # what's after scan_id_*
#     else:
#         # Fallback: we didn't find scan_id_*; use the tail component (could be a colon-encoded "path")
#         rel = Path(s).name

#     # If the "path" is encoded with colons inside the filename, convert to '/'
#     if ":" in rel:
#         rel = rel.replace(":", "/")

#     # Strip leading slashes just in case
#     rel = rel.lstrip("/")

#     # Remove `_beautified` only when immediately before `.js` (case-insensitive)
#     # Fixed regex to handle file extensions properly
#     rel = _re.sub(r"_beautified(?=\.js$)", "", rel, flags=_re.IGNORECASE)
#     #print('rel-path----->',rel)

#     return rel

def normalize_repo_file_path(p: str) -> str:
    """
    Return the complete file path after scan_id_* directory.
    
    Examples:
    - /Users/sundarlalbaror/Downloads/hack/testapi/github/scan_id_kudowswell/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min_beautified.js
        -> cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js
    - /path/scan_id_123/assets/js/file_beautified.js
        -> assets/js/file.js
    - /path/scan_id_345678/assest:dfg:svcg:lazysizes.min.js
        -> assest/dfg/svcg/lazysizes.min.js
    """
    # Normalize slashes
    s = str(p).replace("\\", "/")
    
    # Debug: print the original path
    print(f"üîç Original path: {s}")
    
    # Extract everything after scan_id_* directory (case-insensitive)
    m = _re.search(r"(?i)(?:^|/)scan_id_[^/]+/(.*)$", s)
    if m:
        rel = m.group(1)  # Everything after scan_id_*
        print(f"‚úÖ Extracted path after scan_id: {rel}")
    else:
        # Fallback: if no scan_id found, use the full path
        rel = s
        print(f"‚ö†Ô∏è No scan_id found, using full path: {rel}")
    
    # Replace colons with slashes (for colon-encoded paths)
    if ":" in rel:
        rel = rel.replace(":", "/")
        print(f"üîß After colon replacement: {rel}")
    
    # Strip leading slashes
    rel = rel.lstrip("/")
    
    # Remove _beautified before .js
    rel = _re.sub(r"_beautified(?=\.js$)", "", rel, flags=_re.IGNORECASE)
    
    print(f"üìÅ Final normalized path: {rel}")
    return rel


# def normalize_repo_file_path(p: str) -> str:
#     """
#     Extract final filename INCLUDING internal slashes, then:
#       - strip `_beautified` before `.js`
#       - replace `/` with `:`
#     Example:
#       '/root/scan123/assets/main/a_beautified.js'
#         ‚Üí 'assets:main:a.js'
#     """
#     # 1. Convert to POSIX and extract ONLY the part after scan folder
#     #    (example: "assets/main/foo_beautified.js")
#     p = Path(p).as_posix()
#     parts = p.split("/")     # split full path
#     filename_with_dirs = "/".join(parts[-3:]) if len(parts) > 2 else parts[-1]
#     # we keep last 2-3 segments, not full absolute path

#     # 2. Strip `_beautified.js`
#     filename_with_dirs = _re.sub(r"_beautified(?=\.js$)", "", filename_with_dirs, flags=_re.IGNORECASE)

#     # 3. Replace `/` ‚Üí `:` so DB stores it clean
#     return filename_with_dirs.replace(":", "/")




# ---------------- Chunking / Analysis ----------------
def split_code_to_chunks_with_line_numbers(code_text, max_chunk_tokens=3000, model="gpt-4o"):
    encoder = tiktoken.encoding_for_model(model)
    lines = code_text.splitlines()
    chunks, current_chunk_lines = [], []
    current_token_count = 0
    line_offset = 1

    for line in lines:
        encoded_line = encoder.encode(line + "\n")
        line_token_len = len(encoded_line)
        if current_token_count + line_token_len > max_chunk_tokens and current_chunk_lines:
            chunk_text = "\n".join(current_chunk_lines)
            chunks.append((line_offset - len(current_chunk_lines), chunk_text))
            current_chunk_lines = []
            current_token_count = 0
        current_chunk_lines.append(f"{line_offset}: {line}")
        current_token_count += line_token_len
        line_offset += 1

    if current_chunk_lines:
        chunk_text = "\n".join(current_chunk_lines)
        chunks.append((line_offset - len(current_chunk_lines), chunk_text))
    return chunks

def analyze_code(code_content, prompt_template, github_username="", repo_name="", branch_name="main", repo_file_path=""):
    chunks = split_code_to_chunks_with_line_numbers(code_content)
    encoder = tiktoken.encoding_for_model("gpt-4o")

    for i, (start_line, chunk_text) in enumerate(chunks):
        token_count = len(encoder.encode(chunk_text))
        print(f"Chunk {i+1} tokens: {token_count}")

        prompt = f"{prompt_template}\nAnalyze the following code. Each line is prefixed with a line number for reference:\n\n{chunk_text}"

        for retry_count in range(4):
            try:
                print(f"üì§ Sending chunk {i+1}/{len(chunks)}")
                response = client.chat.completions.create(
                    model=MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    top_p=0.1,
                    max_tokens=8192
                )
                content = response.choices[0].message.content
                chunk_result = extract_json_from_response(content)
                if not isinstance(chunk_result, dict):
                    print(f"‚ö†Ô∏è Skipping chunk {i+1}: Invalid JSON")
                    continue
                #chunk_result["file_path"] = repo_file_path
                chunk_result["file_path"] = normalize_repo_file_path(repo_file_path)


                categorize_and_save(
                    chunk_result,
                    github_username=github_username,
                    repo_name=repo_name,
                    branch_name=branch_name
                )
                time.sleep(3)
                break
            except Exception as e:
                if "429" in str(e) or "rate limit" in str(e).lower():
                    wait = 3 * (retry_count + 1)
                    print(f"‚ö†Ô∏è Rate limit. Retrying after {wait}s")
                    time.sleep(wait)
                else:
                    print(f"‚ùå Error in chunk {i+1}: {e}")
                    break
    return {"status": "chunks_processed"}

# ---------------- Save to DB (issues) ----------------
def categorize_and_save(data, github_username, repo_name, branch_name="main", email=EMAIL, platform="WebApp"):
    result = data.get("result", data)
    print(result)
    #repo_file_path = data.get("file_path", "")
    repo_file_path = normalize_repo_file_path(data.get("file_path", ""))

    created_at = datetime.now()
    risk_analyzer = RiskAnalyzer()

    def generate_mongodb_id(repo_name_, line_number_, created_at_):
        alphabet = string.ascii_lowercase + string.digits
        random_part = ''.join(secrets.choice(alphabet) for _ in range(16))
        timestamp_hex = format(int(created_at_.timestamp()), '08x')
        repo_hash = format(abs(hash(repo_name_)) % (10**8), '06x')
        return f"{timestamp_hex}{repo_hash}{random_part}"[:24]

    def handle_array_field(field_data):
        if isinstance(field_data, list):
            return json.dumps(field_data)
        return field_data

    def normalize_issue(issue, category):
        if not isinstance(issue, dict):
            return None

        line_num = issue.get("line_number", 0)
        mongodb_beacon_id = generate_mongodb_id(repo_name, line_num, created_at)
        risk_analysis = risk_analyzer.analyze_issue_risk(issue)

        ai_severity = issue.get("severity", "Medium")
        risk_level = risk_analysis["risk_level"]
        severity_order = {"Info": 0, "Low": 1, "Medium": 2, "High": 3, "Critical": 4}
        ai_level = severity_order.get(ai_severity, 1)
        risk_level_num = severity_order.get(risk_level, 1)
        final_severity = ai_severity if ai_level <= risk_level_num else risk_level

        base_issue = {
            "username": github_username,
            "email": email,
            "platform": platform,
            "repo_name": repo_name,
            "file_path": repo_file_path,
            "line_number": issue.get("line_number"),
            "vulnerability_type": issue.get("vulnerability_type") or issue.get("issue") or issue.get("issue_type"),
            "cwe": issue.get("cwe", "N/A"),
            "cve": issue.get("cve", ""),
            "severity": final_severity,
            "risk_score": risk_analysis["risk_score"],
            "risk_level": risk_analysis["risk_level"],
            "short_description": issue.get("description") or issue.get("short_description", ""),
            "suggested_fix": issue.get("suggested_fix", "Review the code and apply necessary validation/sanitization."),
            "created_at": created_at,
            "bad_practice": issue.get("bad_practice") if category in ["smelly_code", "malicious_code"] else None,
            "good_practice": issue.get("good_practice") if category in ["smelly_code", "malicious_code"] else None,
            "issueId": issue.get("issue_id") or issue.get("id") or mongodb_beacon_id,
            "branch": branch_name,
            "owasp_2017": issue.get("owasp_2017"),
            "owasp_2021": issue.get("owasp_2021"),
            "reproduction_steps": handle_array_field(issue.get("reproduction_steps")),
            "medium_vapt_summary": issue.get("medium_vapt_summary"),
            "impact": handle_array_field(issue.get("impact")),
            "remediation": handle_array_field(issue.get("remediation")),
            "reference": handle_array_field(issue.get("references"))
        }

        if category == "owasp_security":
            base_issue["bad_practice"] = issue.get("vulnerable_code", "")
            base_issue["good_practice"] = issue.get("patched_code", "")
            base_issue["reproduction_steps"] = base_issue["reproduction_steps"] or json.dumps([])
            base_issue["impact"] = base_issue["impact"] or json.dumps([])
            base_issue["remediation"] = base_issue["remediation"] or json.dumps([])
            base_issue["reference"] = base_issue["reference"] or json.dumps([])

        return base_issue

    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        for category in CATEGORIES:
            section = result.get(category)
            if not section:
                continue

            table_name = CATEGORY_TABLE_MAP[category]
            issues = []

            if isinstance(section, dict):
                for _, issue_list in section.items():
                    for i in issue_list:
                        normalized = normalize_issue(i, category)
                        if normalized:
                            issues.append(normalized)
            elif isinstance(section, list):
                for i in section:
                    normalized = normalize_issue(i, category)
                    if normalized:
                        issues.append(normalized)

            if not issues:
                continue

            keys = issues[0].keys()
            fields = ", ".join(keys)
            placeholders = ", ".join(["%s"] * len(keys))
            insert_query = f"INSERT INTO {table_name} ({fields}) VALUES ({placeholders})"

            for issue in issues:
                cursor.execute(insert_query, list(issue.values()))

            print(f"‚úÖ Inserted {len(issues)} issues into {table_name}")

        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error inserting issues: {e}")
    finally:
        cursor.close()
        conn.close()

# ---------------- SCA helpers ----------------
def detect_ecosystem(filename):
    if filename == "package.json":
        return "npm"
    elif filename == "requirements.txt":
        return "PyPI"
    elif filename == "pom.xml":
        return "Maven"
    return None

def extract_dependencies(filename, content):
    try:
        if filename == "package.json":
            data = json.loads(content)
            deps = data.get("dependencies", {})
            return [{"name": name, "version": version} for name, version in deps.items()]

        elif filename == "requirements.txt":
            lines = content.strip().split("\n")
            result = []
            for line in lines:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if "==" in line:
                    name, version = line.split("==", 1)
                    result.append({"name": name.strip(), "version": version.strip()})
            return result

        elif filename == "pom.xml":
            root = ET.fromstring(content)
            ns = {'m': 'http://maven.apache.org/POM/4.0.0'}
            deps = root.findall(".//m:dependency", ns)
            result = []
            for dep in deps:
                group = dep.find("m:groupId", ns)
                artifact = dep.find("m:artifactId", ns)
                version = dep.find("m:version", ns)
                if group is None or artifact is None or version is None:
                    continue
                result.append({"name": f"{group.text}:{artifact.text}", "version": version.text})
            return result

    except Exception as e:
        print(f"‚ùå Failed to parse dependencies in {filename}: {e}")
    return []

MAX_OSV_RETRIES = 5
OSV_RETRY_DELAY = 3

def query_osv_with_retry(payload):
    for attempt in range(1, MAX_OSV_RETRIES + 1):
        try:
            r = requests.post(OSV_API_URL, json=payload, timeout=15)
            r.raise_for_status()
            return r.json()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå OSV attempt {attempt} failed: {e}")
            if attempt < MAX_OSV_RETRIES:
                time.sleep(OSV_RETRY_DELAY * attempt)
    return None

def save_sca_info(vulns, username, repo, branch, file_path, version, vuln_pack, email=EMAIL, platform="WebApp"):
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        for vuln in vulns:
            cve = next((a for a in vuln.get("aliases", []) if a.startswith("CVE-")), vuln.get("id"))
            cwe = ", ".join(vuln.get("database_specific", {}).get("cwe_ids", []))
            severity = vuln.get("database_specific", {}).get("severity", "Unknown")
            description = vuln.get("details", "")
            vuln_type = vuln.get("summary")
            fix_version = next(
                (e.get("fixed") for r in vuln.get("affected", [{}])[0].get("ranges", [])
                 for e in r.get("events", []) if "fixed" in e),
                "Upgrade Recommended"
            )
            advice_urls = "\n".join(ref.get("url") for ref in vuln.get("references", []) if "url" in ref)
            suggested_fix = f"Upgrade to version {fix_version}.\n\nReferences:\n{advice_urls}"
            affected_versions = ", ".join(vuln.get("affected", [{}])[0].get("versions", [])) or version

            cursor.execute("""
                INSERT INTO sca_info (
                    username, email, platform, repo_name, file_path, line_number,
                    vulnerability_type, cwe, cve, severity, short_description,
                    suggested_fix, vulnerable_code, patched_code,
                    bad_practice, good_practice, issueId, branch, affected_version, vulnerable_package,
                    created_at
                ) VALUES (%s, %s, %s, %s, %s, NULL,
                          %s, %s, %s, %s, %s,
                          %s, NULL, NULL,
                          NULL, NULL, %s, %s, %s, %s,
                          NOW())
            """, (
                username, email, platform, repo, file_path,
                vuln_type, cwe, cve, severity, description,
                suggested_fix, vuln.get("id"), branch, affected_versions, vuln_pack
            ))
        conn.commit()
        print(f"‚úÖ SCA saved for {file_path}: {len(vulns)} vulns")
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error saving SCA for {file_path}: {e}")
    finally:
        cursor.close()
        conn.close()

def run_dependency_scan(file_path, file_content, username, repo, branch, email, platform="WebApp"):
    file_name = Path(file_path).name
    eco = detect_ecosystem(file_name)
    if not eco:
        print(f"‚õî Unsupported SCA file: {file_name}")
        return

    print(f"üîç Running SCA for: {file_name}")
    dependencies = extract_dependencies(file_name, file_content)
    if not dependencies:
        print(f"‚ö†Ô∏è No dependencies found in {file_name}")
        return

    for dep in dependencies:
        pkg = dep.get("name")
        ver = dep.get("version")
        if not pkg or not ver:
            continue
        payload = {"package": {"name": pkg, "ecosystem": eco}, "version": ver}
        data = query_osv_with_retry(payload)
        if not data or "vulns" not in data:
            continue
        vuln_pack = f"{pkg}@{ver}"
        save_sca_info(
            vulns=data["vulns"],
            username=username,
            repo=repo,
            branch=branch,
            file_path=file_path,
            version=ver,
            vuln_pack=vuln_pack,
            email=EMAIL,
            platform=platform
        )

# ---------------- Local Folder Helpers ----------------
# def list_targets(scan_folder: str):
#     """
#     Return (sca_files, js_files). sca_files are exact names in SCA_FILENAMES.
#     js_files prefer *_beautified.js; if none exist, return all js-like files.
#     """
#     scan_folder = os.path.abspath(scan_folder)
#     sca_files = []
#     beautified, plain = [], []

#     for root, _, files in os.walk(scan_folder):
#         for fn in files:
#             full = os.path.join(root, fn)
#             low = fn.lower()

#             # SCA
#             if fn in SCA_FILENAMES:
#                 sca_files.append(full)
#                 continue

#             # JS-like
#             if low.endswith(TARGET_JS_EXTS):
#                 if low.endswith(PREFER_SUFFIX):
#                     beautified.append(full)
#                 else:
#                     plain.append(full)

#     js_files = beautified if beautified else plain
#     return sca_files, js_files

def list_targets(scan_folder: str):
    """
    Return (sca_files, js_files).
    - SCA files are exact SPECIAL_SCA_FILES (always included, even under excluded dirs).
    - JS files: only .js/.mjs/.cjs, prefer *_beautified.js; skip excluded dirs/patterns.
    """
    scan_folder = os.path.abspath(scan_folder)
    sca_files = []
    beautified, plain = [], []

    for root, _, files in os.walk(scan_folder):
        for fn in files:
            full = os.path.join(root, fn)
            posix_full = full.replace("\\", "/")
            suffix = Path(fn).suffix.lower()

            # --- SCA files: include regardless of exclusions ---
            if fn in SPECIAL_SCA_FILES:
                sca_files.append(full)
                continue

            # --- JS-like only (local LLM analysis is JS-centric) ---
            if suffix not in (".js", ".mjs", ".cjs"):
                continue

            # respect exclusions
            if _is_excluded_path(posix_full, fn):
                continue

            low = fn.lower()
            if low.endswith(PREFER_SUFFIX):
                beautified.append(full)
            else:
                plain.append(full)

    js_files = beautified if beautified else plain
    return sca_files, js_files




def read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

"""
def process_local_js(filepath: str, prompt_template: str, username: str, repo_name: str, branch_name: str):
    try:
        code = read_text(filepath)
        rel_path = Path(filepath).as_posix()
        print(f"ü§ñ Analyzing: {rel_path}")

        analysis = analyze_code(
            code_content=code,
            prompt_template=prompt_template,
            github_username=username,
            repo_name=repo_name,
            branch_name=branch_name,
            repo_file_path=rel_path,
        )
        if not isinstance(analysis, dict):
            print(f"‚ùå Skipping {rel_path}: Invalid result")
            return f"‚ùå Invalid: {rel_path}"

        analysis["file_path"] = rel_path
        categorize_and_save(
            analysis,
            github_username=username,
            repo_name=repo_name,
            branch_name=branch_name,
        )
        time.sleep(random.uniform(2.0, 5.0))
        return f"‚úÖ Processed: {rel_path}"
    except Exception as e:
        return f"‚ùå Error: {rel_path} ({e})"

"""

# def process_local_js(filepath: str, prompt_template: str, username: str, repo_name: str, branch_name: str):
#     try:
#         code = read_text(filepath)
#         # previously: rel_path = Path(filepath).as_posix()
#         rel_path = normalize_repo_file_path(filepath)  # ‚Üê only filename, beautified stripped
#         print(f"ü§ñ Analyzing: {rel_path}")

#         analysis = analyze_code(
#             code_content=code,
#             prompt_template=prompt_template,
#             github_username=username,
#             repo_name=repo_name,
#             branch_name=branch_name,
#             repo_file_path=rel_path,   # ‚Üê pass normalized name
#         )
        
#         analysis["file_path"] = rel_path  # ‚Üê normalized name
#         categorize_and_save(analysis, github_username=username, repo_name=repo_name, branch_name=branch_name)

def process_local_js(filepath: str, prompt_template: str, username: str, repo_name: str, branch_name: str):
    rel_path = normalize_repo_file_path(filepath)  # only filename, strip *_beautified
    try:
        code = read_text(filepath)
        print(f"ü§ñ Analyzing: {rel_path}")

        analysis = analyze_code(
            code_content=code,
            prompt_template=prompt_template,
            github_username=username,
            repo_name=repo_name,
            branch_name=branch_name,
            repo_file_path=rel_path,
        )

        # Ensure a dict shape before saving
        if not isinstance(analysis, dict):
            return f"‚ö†Ô∏è Skipped (invalid analysis): {rel_path}"

        analysis["file_path"] = rel_path
        categorize_and_save(analysis, github_username=username, repo_name=repo_name, branch_name=branch_name)
        return f"‚úÖ Processed: {rel_path}"

    except Exception as e:
        return f"‚ùå Error processing {rel_path}: {e}"


#---------------- Scan Status ----------------
def update_scan_status(scan_id, status="completed"):
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        try:
            scan_id_int = int(str(scan_id))
        except ValueError:
            scan_id_int = None
        if scan_id_int is None:
            cursor.execute("""
                UPDATE sastcode_schema.scan_status
                SET status = %s, finished_at = NOW()
                WHERE scan_id::text = %s
            """, (status, str(scan_id)))
        else:
            cursor.execute("""
                UPDATE sastcode_schema.scan_status
                SET status = %s, finished_at = NOW()
                WHERE scan_id = %s
            """, (status, scan_id_int))
        conn.commit()
        print(f"‚úÖ scan_status ‚Üí '{status}' for scan_id {scan_id}")
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error updating scan_status: {e}")
    finally:
        cursor.close()
        conn.close()



# ---------------- Main ----------------
def main():
    if len(sys.argv) < 3:
        print("Usage: python WebPentestScanner.py <username> <scan_id> [domain_name]")
        sys.exit(1)

    username = sys.argv[1]
    scan_id = sys.argv[2]
    domain_name = sys.argv[3] if len(sys.argv) > 3 else None

    scan_folder = os.path.abspath(scan_id)
    repo_name = domain_name if domain_name else scan_id
    branch = "main"

    print("üîé Local SCA + JS analysis")
    print(f"  user      : {username}")
    print(f"  scan_id   : {scan_id}")
    print(f"  repo_name : {repo_name}")
    print(f"  folder    : {scan_folder}")
    print(f"  branch    : {branch}")

    try:
        sca_files, js_files = list_targets(scan_folder)
        if not sca_files and not js_files:
            print("‚ö†Ô∏è No files found to analyze in this scan folder.")
            update_scan_status(scan_id, status="completed")
            return

        # Get token/email (for DB fields) if your API expects it; safe to skip if not needed
        try:
            # OPTIONAL: if your DB inserts need EMAIL, fetch it here
            # Keeping same behavior as your previous pipeline (SITE_URL/getToken)
            url = f"{SITE_URL}/getToken"
            resp = requests.post(url, json={"username": username, "platform": "WebApp"}, timeout=10)
            if resp.ok:
                data = resp.json().get("data", {})
                global EMAIL, TOKEN
                EMAIL = data.get("email", EMAIL)
                TOKEN = data.get("client_access_token", TOKEN)
        except Exception as _:
            pass

        # --- SCA first (sequential; not heavy) ---
        if sca_files:
            print(f"üß™ SCA files: {len(sca_files)}")
            for fp in sca_files:
                try:
                    content = read_text(fp)
                    #rel = Path(fp).as_posix()
                    #run_dependency_scan(rel, content, username, repo_name, branch, email=EMAIL, platform="WebApp")
                    rel = normalize_repo_file_path(fp)  # ‚Üê only filename
                    run_dependency_scan(rel, content, username, repo_name, branch, email=EMAIL, platform="WebApp")

                except Exception as e:
                    print(f"‚ùå SCA error {fp}: {e}")

        # --- JS analysis (parallel) ---
        if js_files:
            print(f"üß† JS files: {len(js_files)}")
            prompt_template = load_prompt_template()
            threads = min(MAX_THREADS, max(1, len(js_files)))
            print(f"üöÄ Processing JS with {threads} thread(s)...")

            with ThreadPoolExecutor(max_workers=threads) as ex:
                futs = [ex.submit(process_local_js, f, prompt_template, username, repo_name, branch) for f in js_files]
                for fut in as_completed(futs):
                    print(fut.result())
        else:
            print("‚ÑπÔ∏è No JS files to analyze.")

        print("‚úÖ All tasks finished.")
        update_scan_status(scan_id, status="completed")

    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        update_scan_status(scan_id, status="failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
